{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:52:58.046599Z",
     "start_time": "2023-07-18T10:52:55.346683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:52:58.051039Z",
     "start_time": "2023-07-18T10:52:58.049124Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0               word      lemma pos_tag  freq  word_length  \\\n0          1104841         მონოზონისა   მონოზონი       N     1           10   \n1          1260770       სამხრეთიდგან   სამხრეთი       N     1           12   \n2           637615       მოფიქრებაშია  მოფიქრება       N     1           12   \n3            79017           სიდიადეს    სიდიადე       N   209            8   \n4          1107010  განგსტერებისათვის  განგსტერი       N     2           17   \n...            ...                ...        ...     ...   ...          ...   \n956889      485527             ტონგას      ტონგა       N   102            6   \n956890      113442              ადენა      ადენა       N     5            5   \n956891      131905              ვრაცხ    *რაცხვა       V    46            5   \n956892     1049224          ამოდგომაც   ამოდგომა       N     1            9   \n956893      652185         ცხიმისაგან      ცხიმი       N     8           10   \n\n        lemma_length     ratio lemma_length_category word_length_category  \\\n0                  8  1.250000                medium               medium   \n1                  8  1.500000                medium                 high   \n2                  9  1.333333                medium                 high   \n3                  7  1.142857                   low                  low   \n4                  9  1.888889                medium                 high   \n...              ...       ...                   ...                  ...   \n956889             5  1.200000                   low                  low   \n956890             5  1.000000                   low                  low   \n956891             7  0.714286                   low                  low   \n956892             8  1.125000                medium                  low   \n956893             5  2.000000                   low               medium   \n\n       ratio_category  reported_speech freq_category  \n0             greater            False           low  \n1             greater            False           low  \n2             greater            False           low  \n3             greater            False          high  \n4             greater            False           low  \n...               ...              ...           ...  \n956889        greater            False          high  \n956890          equal            False        medium  \n956891           less            False        medium  \n956892        greater            False           low  \n956893        greater            False        medium  \n\n[956894 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>word</th>\n      <th>lemma</th>\n      <th>pos_tag</th>\n      <th>freq</th>\n      <th>word_length</th>\n      <th>lemma_length</th>\n      <th>ratio</th>\n      <th>lemma_length_category</th>\n      <th>word_length_category</th>\n      <th>ratio_category</th>\n      <th>reported_speech</th>\n      <th>freq_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1104841</td>\n      <td>მონოზონისა</td>\n      <td>მონოზონი</td>\n      <td>N</td>\n      <td>1</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1.250000</td>\n      <td>medium</td>\n      <td>medium</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1260770</td>\n      <td>სამხრეთიდგან</td>\n      <td>სამხრეთი</td>\n      <td>N</td>\n      <td>1</td>\n      <td>12</td>\n      <td>8</td>\n      <td>1.500000</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>637615</td>\n      <td>მოფიქრებაშია</td>\n      <td>მოფიქრება</td>\n      <td>N</td>\n      <td>1</td>\n      <td>12</td>\n      <td>9</td>\n      <td>1.333333</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>79017</td>\n      <td>სიდიადეს</td>\n      <td>სიდიადე</td>\n      <td>N</td>\n      <td>209</td>\n      <td>8</td>\n      <td>7</td>\n      <td>1.142857</td>\n      <td>low</td>\n      <td>low</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1107010</td>\n      <td>განგსტერებისათვის</td>\n      <td>განგსტერი</td>\n      <td>N</td>\n      <td>2</td>\n      <td>17</td>\n      <td>9</td>\n      <td>1.888889</td>\n      <td>medium</td>\n      <td>high</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>956889</th>\n      <td>485527</td>\n      <td>ტონგას</td>\n      <td>ტონგა</td>\n      <td>N</td>\n      <td>102</td>\n      <td>6</td>\n      <td>5</td>\n      <td>1.200000</td>\n      <td>low</td>\n      <td>low</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>956890</th>\n      <td>113442</td>\n      <td>ადენა</td>\n      <td>ადენა</td>\n      <td>N</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1.000000</td>\n      <td>low</td>\n      <td>low</td>\n      <td>equal</td>\n      <td>False</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>956891</th>\n      <td>131905</td>\n      <td>ვრაცხ</td>\n      <td>*რაცხვა</td>\n      <td>V</td>\n      <td>46</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.714286</td>\n      <td>low</td>\n      <td>low</td>\n      <td>less</td>\n      <td>False</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>956892</th>\n      <td>1049224</td>\n      <td>ამოდგომაც</td>\n      <td>ამოდგომა</td>\n      <td>N</td>\n      <td>1</td>\n      <td>9</td>\n      <td>8</td>\n      <td>1.125000</td>\n      <td>medium</td>\n      <td>low</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>956893</th>\n      <td>652185</td>\n      <td>ცხიმისაგან</td>\n      <td>ცხიმი</td>\n      <td>N</td>\n      <td>8</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2.000000</td>\n      <td>low</td>\n      <td>medium</td>\n      <td>greater</td>\n      <td>False</td>\n      <td>medium</td>\n    </tr>\n  </tbody>\n</table>\n<p>956894 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('csv/train.csv')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:00.645902Z",
     "start_time": "2023-07-18T10:52:58.052483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "32"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = data.word.str.len().max()\n",
    "MAX_LENGTH"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:01.226549Z",
     "start_time": "2023-07-18T10:53:00.715985Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_tokens(t):\n",
    "    yield from list(t)\n",
    "\n",
    "UNK_TOKEN = '<unk>'\n",
    "EOW_TOKEN = '<end>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOW_TOKEN = '<start>'\n",
    "\n",
    "vocab = build_vocab_from_iterator(iterator=get_tokens(itertools.chain(data['word'], data['lemma'])),\n",
    "                                  specials=[EOW_TOKEN, UNK_TOKEN, PAD_TOKEN, SOW_TOKEN],\n",
    "                                  special_first=False\n",
    "                                  )\n",
    "vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "\n",
    "def word_to_seq(word: str):\n",
    "    return torch.tensor(vocab(list(word)) + [vocab[EOW_TOKEN]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:04.887258Z",
     "start_time": "2023-07-18T10:53:01.225039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "RANDOM_STATE = 42"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:04.890081Z",
     "start_time": "2023-07-18T10:53:04.888308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class LemmaDataSet(Dataset):\n",
    "    def __init__(self, data_frame: pd.DataFrame):\n",
    "        self.data = data_frame\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> (str, str):\n",
    "        row = self.data.iloc[idx]\n",
    "        return row['word'], row['lemma']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:04.895573Z",
     "start_time": "2023-07-18T10:53:04.893598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def coallate_words(batch):\n",
    "    word_list, lemma_list = [], []\n",
    "    for word, lemma in batch:\n",
    "        word_list.append(torch.tensor(word_to_seq(word)))\n",
    "        lemma_list.append(torch.tensor(word_to_seq(word)))\n",
    "\n",
    "\n",
    "    return torch.tensor(pad_sequence(word_list, batch_first=True, padding_value=vocab[PAD_TOKEN])), torch.tensor(pad_sequence(lemma_list, batch_first=True, padding_value=vocab[PAD_TOKEN]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:04.899832Z",
     "start_time": "2023-07-18T10:53:04.898236Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, random_state=RANDOM_STATE, train_size=0.8, shuffle=True)\n",
    "train_loader = DataLoader(LemmaDataSet(train_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=coallate_words)\n",
    "val_loader = DataLoader(LemmaDataSet(val_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=coallate_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.103Z",
     "start_time": "2023-07-18T10:53:04.903645Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim,  hidden_dim, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        # set dimensions\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embedding_size = emb_dim\n",
    "        self.vocab_size = vocab_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # initialize layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.gru = nn.GRU(self.embedding_size, self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.109911Z",
     "start_time": "2023-07-18T10:53:05.108424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, vocab_dim, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # set dimensions\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embedding_size = emb_dim\n",
    "        self.output_size = vocab_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #initialize layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size,self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedding = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedding, hidden)\n",
    "        pred = self.softmax(self.out(output))\n",
    "        return pred, hidden\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.115698Z",
     "start_time": "2023-07-18T10:53:05.113926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target=None, teacher_forcing_ratio=0.5):\n",
    "        input_length = source.size(1) #get the input length (number of words in sentence)\n",
    "        batch_size = source.size(0)\n",
    "        vocab_size = self.encoder.vocab_size\n",
    "\n",
    "        #initialize a variable to hold the predicted outputs\n",
    "        outputs = []\n",
    "\n",
    "        #encode every word in a sentence\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(source[:, i].unsqueeze(1))\n",
    "\n",
    "        #use the encoder’s hidden layer as the decoder hidden\n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "        #add a token before the first predicted word\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(vocab[SOW_TOKEN])\n",
    "\n",
    "        #topk is used to get the top K value over a list\n",
    "        #predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value.\n",
    "\n",
    "        for t in range(target.size(1) if target is not None else MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs.append(decoder_output)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            if target is not None and target.size(1) > t:\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                target_input = target[:, t].unsqueeze(1)\n",
    "                decoder_input = (target_input if teacher_force else topi.squeeze(-1).detach())\n",
    "            else:\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        return torch.cat(outputs, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.136833Z",
     "start_time": "2023-07-18T10:53:05.122130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/ks4jxwf13hn5277w8lg01sfw0000gn/T/ipykernel_59781/3941047154.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_list.append(torch.tensor(word_to_seq(word)))\n",
      "/var/folders/27/ks4jxwf13hn5277w8lg01sfw0000gn/T/ipykernel_59781/3941047154.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lemma_list.append(torch.tensor(word_to_seq(word)))\n",
      "/var/folders/27/ks4jxwf13hn5277w8lg01sfw0000gn/T/ipykernel_59781/3941047154.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(pad_sequence(word_list, batch_first=True, padding_value=vocab[PAD_TOKEN])), torch.tensor(pad_sequence(lemma_list, batch_first=True, padding_value=vocab[PAD_TOKEN]))\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[18, 14, 17,  ...,  0, 16, 18],\n        [18,  6, 17,  ...,  0,  4, 18],\n        [ 2,  6,  8,  ...,  0, 16,  1],\n        ...,\n        [18, 14,  1,  ...,  0, 16, 18],\n        [ 8,  9,  3,  ...,  0, 16, 18],\n        [ 1,  6,  7,  ...,  0, 16, 18]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(EncoderRNN(vocab_dim=len(vocab), emb_dim=64 ,hidden_dim=128).to(device), DecoderRNN(vocab_dim=len(vocab), emb_dim=64, hidden_dim=128).to(device), device=device)\n",
    "source, target = next(iter(train_loader))\n",
    "model(source.to(device), target.to(device)).argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.304314Z",
     "start_time": "2023-07-18T10:53:05.126750Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for data in dataloader:\n",
    "        i += 1\n",
    "        if i == 200:\n",
    "            break\n",
    "        input_tensor, target_tensor = data\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_outputs = model(input_tensor, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:05.309292Z",
     "start_time": "2023-07-18T10:53:05.307308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:21.298442Z",
     "start_time": "2023-07-18T10:53:21.295927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:21.965401Z",
     "start_time": "2023-07-18T10:53:21.301026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, learning_rate=0.001,\n",
    "          print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 1:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:21.971337Z",
     "start_time": "2023-07-18T10:53:21.969028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_epoch(train_loader, model.to(device))\n",
    "train(train_loader, model.to(device), 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:21.978794Z",
     "start_time": "2023-07-18T10:53:21.974010Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T10:53:21.980400Z",
     "start_time": "2023-07-18T10:53:21.977128Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
